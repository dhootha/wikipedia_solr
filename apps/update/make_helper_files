#!/bin/bash
set -e
. ~/.profile
echo "make_helper_files:STARTING"
FULL_BZ2=$GC_WIKIPEDIA_DL_DIR/enwiki-latest-pages-articles.xml.bz2
NAMED_PIPE_DIR=$GC_WIKIPEDIA_REPO_ROOT/named_pipes

touch $NAMED_PIPE_DIR
rm -rf  $NAMED_PIPE_DIR
mkdir $NAMED_PIPE_DIR




UNCOMP1=$NAMED_PIPE_DIR/uncomp1.xml
UNCOMP2=$NAMED_PIPE_DIR/uncomp2.xml
UNCOMP3=$NAMED_PIPE_DIR/uncomp3.xml

touch  $UNCOMP1 $UNCOMP2 $UNCOMP3
rm $UNCOMP1 $UNCOMP2 $UNCOMP3
mkfifo $UNCOMP1 $UNCOMP2 $UNCOMP3

ENDINGS=$GC_WIKIPEDIA_DL_DIR/page_endings.txt
STARTS=$GC_WIKIPEDIA_DL_DIR/page_starts.txt
TITLES=$GC_WIKIPEDIA_DL_DIR/page_titles.txt

echo "starting bunzip to tees"
# here we create the page endings file
nohup bunzip2 -c $FULL_BZ2 | tee $UNCOMP1 | tee $UNCOMP2 > $UNCOMP3 &


echo "starting the greps, this should take about an hour"
# we need to pick off the fifos in reverse order so that we don't get
# a pipe stall
grep -n "<title>" $UNCOMP3 > $TITLES &
grep -n "</page>" $UNCOMP2 > $ENDINGS &
grep -n "<page>"  $UNCOMP1 > $STARTS 


# we don't background this one, all of these processes should run at
# about the same pace, we don't want one named pipe blowing up with a
# buffer or the other tees to block, this should keep things moving smoothly

echo "finding the  1k, 10k, and 100k article endings "
#grab the line number of the 1000'd page ending
page_1_000=`cat $ENDINGS   | head -n 1000 | cut  -d":" -f1 | tail -n 1`
page_10_000=`cat $ENDINGS  | head -n 10000 | cut  -d":" -f1 | tail -n 1`
page_100_000=`cat $ENDINGS | head -n 100000 | cut  -d":" -f1 | tail -n 1`

echo "making the 1k file"
bunzip2 -c $FULL_BZ2 | head -n  $page_1_000 >    $GC_WIKIPEDIA_DL_DIR/first_1_000.xml
echo "making the 10k file"
bunzip2 -c $FULL_BZ2 | head -n  $page_10_000 >   $GC_WIKIPEDIA_DL_DIR/first_10_000.xml
echo "making the 100k file"
bunzip2 -c $FULL_BZ2 | head -n  $page_100_000 >  $GC_WIKIPEDIA_DL_DIR/first_100_000.xml

# lets clean up after ourselves
rm $UNCOMP1 $UNCOMP2 $UNCOMP3
echo "make_helper_files:FINISHED"
