
* Introduction
The wikipedia_solr system is a locally installed full text index of
the wikipedia.  It allows fast searches through the entire wikipedia.
  
* Installation
** prerequisites

*** java 6 
*** apache ant installed
*** git command line tools installed
the git command line tools must be installed on your machine, along
with ssh key access to the github repo.  You can read more information
about setting ssh keys here

[[http://help.github.com/set-up-git-redirect/]]



*** space requirements
the source code and compiled files take about 700 Mb.
a minimum index of only the first 1000 articles will take about
50-60megs

the complete index takes 16 Gb when finished, and up to 34 Gb while the
indexing process is running

the complete wikipedia data dump file is 8 Gb

The greatest amount of space that this system could take on your
system is

43 Gb = 8 + 34 + 1

You can store all three components on separate drives


** installation instructions
in a terminal run these commands, they will install the wikipedia_solr
sub-directory in the current directory.  They will also add three
lines to your ~/.profile


#+BEGIN_SRC shell
git clone  git@github.com:General-Cybernetics/wikipedia_solr.git
cd wikipedia_solr
echo "######## lines after this added by wikipedia_solr ######"  >> ~/.profile
echo "export GC_WIKIPEDIA_REPO_ROOT=\"`pwd`\" " >> ~/.profile
echo "export GC_WIKIPEDIA_DL_DIR=\"`pwd`/dl_dir\" " >> ~/.profile
echo "export GC_WIKIPEDIA_SOLR_DATA_DIR=\"`pwd`/solr_data_dir\" " >> ~/.profile
./apps/update/update_all
open ./apps
#+END_SRC

when these steps have finished a finder window pointing to the apps
directory will open up.  In here you will find maintence applications

* running wikipedia_solr

** apps
there are a set of apps which allow you to perform most regular
maintence tasks.

*** start_solr/start_solr_wrap.app
this app starts the solr server and leaves a terminal window showing
the current server log

*** update/update_all_wrap.app
this app pulls all the latest code and recompiles every module
necessary for the system.  

*** update/download_dump_wrap.app
this app downloads the latest wikipedia dump into your
$GC_WIKIPEDIA_DL_DIR , it is necessary to have a wikipedia dump to
index articles


*** reindex/index_1000_wrap.app
this app pulls in the first 1000 articles, it is useful as a sanity
check when modifying the indexing scheme

*** reindex/index_all_wrap.app
this app indexes the entire wikipedia into solr.  It takes a long
time, about 12 hours on a macbook pro

** common workflows
*** I want to reindex the entire wikipedia with newer parsing code
First close the existing start_solr window, solr will have to be
restarted to source the new code

run the following scripts or apps the apps are the same as the
scripts, except they are followed by "_wrap.app" for instance


"update_all"->"update_all_wrap.app".  

#+BEGIN_SRC shell
cd $GC_WIKIPEDIA_REPO_ROOT/apps

# we need to pull the most recent code and compile it
./update/update_all
# we need to start the server so that it is reading the most recent
# codebase
./start_solr/start_solr
# now kickoff the reindex
./reindex/index_all

#+END_SRC


** system setup
*** how do I control the location where solr stores the index files
edit your ~/.profile
change the location specified in the line
#+BEGIN_SRC shell
export GC_WIKIPEDIA_SOLR_DATA_DIR="/Volumes/LaCie_1/data/index_wikipedia"
#+END_SRC

*** how do I control where the wikipedia dump file is downloaded?
edit your ~/.profile
change the location specified in the line
#+BEGIN_SRC shell
export GC_WIKIPEDIA_DL_DIR="/Volumes/LaCie_1/data/data_wikipedia"
#+END_SRC






* Querying

The solr system is queried over http, results can be returned in json
format or xml format.  all examples are given using the json format.


** breakdown of a query url
http://localhost:8983/solr/select/?q=articlePlainText%3A%22american%22&version=2.2&start=0&rows=1000&indent=on&wt=json

*** q parameter
the q parameter is the actual query, unurlescaped this query looks
articlePlainText:"american"

this tells solr to search the 'articlePlainText' field in the entire database for the term
american.

*** version parameter
the 'version' parmeter is of unknown consequence, use a value of 2.2 for
continuity

*** start parameter
the start parameter controls the first row the result set to be
returned

*** rows parameter
the rows parameter controls how many documents (at most) to return
after the start document.

*** indent parameter
the indent=on causes solr to pretty print the result.

*** wt parameter
the wt=json causes solr to return the result in json format.

** interactive tour of query formation with solr
*** complex queries - phrases ANDs ORs NOTs

take a look at py/query_demo.py to see this as a running program


note qp takes the un-urlencoded q parameter as input, it executes the
query and prints some simple stats about it,
including the complete formed url, it returns the total number of
documents found for that query .

triple quotes are a python convention for encoding multiline strings
or quote containing strings.  the value of a triple quoted string is
between the first triple quote and last triple quote.

a string preceded by a 'u' is a unicode string

the leading and trailing  space 

assert is a python statement that throws an error when it is give a
false value, none of these asserts throw an error


****  search for american with quotes surounding
#+BEGIN_SRC py
american = qp(''' articlePlainText:"american" ''')
#+END_SRC

|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3A%22american%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|1|
|params|{u'q': u'articlePlainText:"american"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|619399|


****  search for american without surrounding quotes
#+BEGIN_SRC py
american_no_quote = qp(''' articlePlainText:american ''')
#+END_SRC
note - for single terms, we got the same number of documents back when we
quoted "american" as we got back when we didn't quote "american"

| solr url |     [[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican&start=0&rows=10&indent=on&wt=json]] |
| QTime    |                                                                                                      0 |
| params   | {u'q': u'articlePlainText:american', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'} |
| numFound |                                                                                                 619399 |

****  search for american without leading/trailing space
#+BEGIN_SRC py
american_no_trail = qp('''articlePlainText:american''')
#+END_SRC


|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican&start=0&rows=10&indent=on&wt=json]]|
|QTime|0|
|params|{u'q': u'articlePlainText:american', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|619399|


**** syntax verification
#+BEGIN_SRC py
assert american == american_no_quote
assert american_no_trail == american_no_quote
#+END_SRC


**** search for 'samoa' get 4755 docs
#+BEGIN_SRC py
samoa =qp(''' articlePlainText:samoa ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Asamoa&start=0&rows=10&indent=on&wt=json]]|
|QTime|186|
|params|{u'q': u'articlePlainText:samoa', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|4755|


****  search for 'american' or 'samoa'  get 621,927 docs
#+BEGIN_SRC py
american_or_samoa = qp(''' articlePlainText:american OR _query_:"articlePlainText:samoa" ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican+OR+_query_%3A%22articlePlainText%3Asamoa%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|191|
|params|{u'q': u'articlePlainText:american OR _query_:"articlePlainText:samoa"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|621,927|

****  search for documents containing 'american' and 'samoa' -> 2227
#+BEGIN_SRC py
american_and_samoa = qp(''' articlePlainText:american AND  _query_:"articlePlainText:samoa" ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican+AND+_query_%3A%22articlePlainText%3Asamoa%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|183|
|params|{u'q': u'articlePlainText:american AND _query_:"articlePlainText:samoa"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|2227|


**** search for docs containg 'samoa' but not containing 'american' ->2528
#+BEGIN_SRC py
samoa_not_american = qp(''' articlePlainText:samoa NOT _query_:"articlePlainText:american" ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Asamoa+NOT+_query_%3A%22articlePlainText%3Aamerican%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|46|
|params|{u'q': u'articlePlainText:samoa NOT _query_:"articlePlainText:american"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|2528|

**** search for the phrase "american samoa" -> 1397
#+BEGIN_SRC py
american_samoa_phrase = qp('''articlePlainText:"american samoa"''')
#+END_SRC

|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3A%22american+samoa%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|1|
|params|{u'q': u'articlePlainText:"american samoa"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|1397|


**** proof of system consitency
#+BEGIN_SRC py
assert american_or_samoa == (american + samoa_not_american)
assert 621927            == (619399   + 2528)

assert american_and_samoa >= american_samoa_phrase
assert 2227               >=     1397
#+END_SRC

**** double phrase AND query
#+BEGIN_SRC py
 a =qp(''' articlePlainText:"american samoa" AND  _query_:"articlePlainText:'manifest destiny'" ''')
#+END_SRC

|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3A%22american+samoa%22+AND+_query_%3A%22articlePlainText%3A%27manifest+destiny%27%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|199|
|params|{u'q': u'articlePlainText:"american samoa" AND _query_:"articlePlainText:\'manifest destiny\'"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|4 |


**** ambiguous syntax
note the following syntax query is unclear and I can't decipher the
results, don't issue queries like this, the results are undecided  and
unsupported by me 
#+BEGIN_SRC py
a =qp('''articlePlainText:american samoa''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican+samoa&start=0&rows=10&indent=on&wt=json]]|
|QTime|73|
|params|{u'q': u'articlePlainText:american samoa', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|619,399|

** additional query formation resources
If you want more information about solr query syntax, try thes resources

nested queries in solr
[[http://www.lucidimagination.com/blog/2009/03/31/nested-queries-in-solr/]]

the solr-wiki page, not actually that helpful
[[http://wiki.apache.org/solr/SolrQuerySyntax]]

* Implementation notes


** overview
this wikipedia search system uses solr [[http://wiki.apache.org/solr/]]
and the jwpl wikimedia markup parsing library
[[http://code.google.com/p/jwpl/]].

I used the DataImportHandler framework to import the XML wikipedia
dump.  I wrote a custom transformer that integrates into the
DataImportHandlerFramework, this handler calls the jwpl parsing
library to extract the article text from the wikimedia markup

I modified solr in two places.  First I changed the file reader so
that it will read from a named pipe.  This allows us to keep the
article dump compressed on disk, allowing for faster I/O and less disk
usage

I also modified the xml reader so that it doesn't kill an entire
import if there is a missing xml tag.  This extra fault tolerance
ensures that hours of work aren't lost.  Wikipedia article dumps are
of the format described in [[http://www.mediawiki.org/xml/export-0.5.xsd]]. 
 The downloaded dumps seem to be missing the final closing
</mediawiki> tag.  We could compare md5sums if we are worried about
integrity.



*** custom code
the custom code I wrote for this project can be found in 

**** transformer
[[https://github.com/General-Cybernetics/wikipedia_solr/blob/master/solr_home/Wikipedia_importer/wikipedia_solr/src/wikipedia_solr/WikimediaToTextTransformer.java]
[solr_home/Wikipedia_importer/wikipedia_solr/src/wikipedia_solr/]]

**** named pipe file reader
[[https://github.com/paddymul/lucene-solr/blob/06a176316bba15bf6967c87d3799ef743067e972/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileDataSource.java][
lib/solr/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileDataSource.java]]


**** tolerant xml reader
[[https://github.com/paddymul/lucene-solr/blob/06a176316bba15bf6967c87d3799ef743067e972/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java][
lib/solr/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java]]


** solr configuration
*** schema configuration

the solr [[ http://wiki.apache.org/solr/SchemaXml ][ schema.xml]]
for this project can be found  
[[https://github.com/General-Cybernetics/wikipedia_solr/blob/master/solr_home/solr/conf/schema.xml][
solr_home/solr/conf/schema.xml]]

**** field explanation
[[http://wiki.apache.org/solr/SchemaXml#Fields]]

each field can have one of multiple flags applied to it
***** stored
a stored field has its original version saved by lucene
***** indexed
an indexed field can be searched against


**** our schema

this controls which fields are stored and indexed.  We have a very
simple schema, only three relevant fields, title, articlePlainText and
sectionParsed.

***** articlePlainText
articlePlainText is the field that is searched on, it is an indexed
version of the plaintext of each wikipedia article.  It isn't stored
since the plaintext on its own isn't that useful

***** sectionParsed
this field is stored, but not indexed.  it is a json-string
[[http://www.json.org/]] of article
sections, in the form of 

[{"section_name":["paragraph1", "paragraph2"]}, {"another section
title": ["paragph1", "p2"]}]



*** solr-config.xml


the [[http://wiki.apache.org/solr/SolrConfigXml][solr-config]] for this project can be found here
[[https://github.com/General-Cybernetics/wikipedia_solr/blob/master/solr_home/solr/conf/solrconfig.xml][solr_home/solr/conf/solrconfig.xml]]

It stays pretty close to the example config, except for additional
java properties that it reads, which allow the system to be more
easily configured

*** maintence scripts

there are a variety of maintence scripts that can be found in apps/* ,
they are explained in the readme.org




