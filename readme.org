
* Introduction
The wikipedia_solr system is a locally installed full text index of
the wikipedia.  It allows fast searches through the entire wikipedia.
  
* Installation
** Prerequisites

*** Java 6 
Java 6 is installed by default on OS X 10.6 and above.
*** Apache Ant installed
[[http://ant.apache.org/manual/install.html][The Apache Ant officialinstallation guide]] 

*** git command line tools installed
The git command line tools must be installed on your machine, along
with ssh key access to the github repo.  You can read more information
about setting ssh keys here:

[[http://help.github.com/set-up-git-redirect/]]



*** space requirements
The source code and compiled files take about 700 Mb.
a minimum index of only the first 1000 articles will take about
50-60 megs.

The complete index takes 16 Gb when finished, and up to 34 Gb while the
indexing process is running.

The complete wikipedia data dump file is 8 Gb.

The greatest amount of space that this system could take on your
system is

43 Gb = 8 + 34 + 1

You can store all three components on separate drives.


** Installation Instructions
In a terminal run these commands, they will install the wikipedia_solr
sub-directory in the current directory.  They will also add three
lines to your ~/.profile .


#+BEGIN_SRC shell
git clone  git@github.com:General-Cybernetics/wikipedia_solr.git
cd wikipedia_solr
echo "######## lines after this added by wikipedia_solr ######"  >> ~/.profile
echo "export GC_WIKIPEDIA_REPO_ROOT=\"`pwd`\" " >> ~/.profile
echo "export GC_WIKIPEDIA_DL_DIR=\"`pwd`/dl_dir\" " >> ~/.profile
echo "export GC_WIKIPEDIA_SOLR_DATA_DIR=\"`pwd`/solr_data_dir\" " >> ~/.profile
./apps/update/update_all
open ./apps
#+END_SRC

When these steps have finished a finder window pointing to the apps
directory will open up.  In here you will find maintence applications.

* Running wikipedia_solr

** Apps
there are a set of apps which allow you to perform most regular
maintence tasks.

*** start_solr/start_solr_wrap.app
This app starts the solr server and leaves a terminal window showing
the current server log.  Any code changes, or file location changes
(the variables specified in ~/.profile) require restarting the
server before the server recognizes the new code.  

*** update/update_all_wrap.app
This app pulls all the latest code and recompiles every module
necessary for the system.

*** update/download_dump_wrap.app
This app downloads the latest wikipedia dump into your
$GC_WIKIPEDIA_DL_DIR , it is necessary to have a wikipedia dump to
index articles.


*** reindex/index_1000_wrap.app
This app pulls in the first 1000 articles, it is useful as a sanity
check when modifying the indexing scheme, and a way of quickly trying
new indexing/text processing techniques. Running this app blows away
the current index, so you might want to change your
GC_WIKIPEDIA_SOLR_DATA_DIR location to keep an existing large
index as a backup.

*** reindex/index_all_wrap.app
This app indexes the entire wikipedia into solr.  It takes a long
time, about 12 hours on a macbook pro. Running this app blows away
the current index, so you might want to change your
GC_WIKIPEDIA_SOLR_DATA_DIR location to keep an existing large
index as a backup.

** Common Workflows

*** I want to run a new index, but not lose an existing index.
Solr stores indexes in the path controlled by the
GC_WIKIPEDIA_SOLR_DATA_DIR environment variable.

edit the line of your ~/.profile that looks like this
#+BEGIN_SRC shell
export GC_WIKIPEDIA_SOLR_DATA_DIR="/Volumes/LaCie_1/data/index_wikipedia"
#+END_SRC
to point to another location.  Changes to this location require a
restart of the server to be recognized.

*** I want to reindex the entire wikipedia with newer parsing code
First close the existing start_solr window, solr will have to be
restarted to use the new code.

Run the following scripts or apps.  The apps have the same name  as the
scripts, except they are followed by "_wrap.app", to run them, you can
double-click on the .app from finder.  

From the repo root, run the following scripts or apps

1:  Pull the most recent code and compile it.
#+BEGIN_SRC shell        
./update/update_all        
#+END_SRC
2:  Start the server so that it is reading the most recent codebase.
#+BEGIN_SRC shell
./start_solr/start_solr
#+END_SRC
3:  Next kickoff the reindex.
#+BEGIN_SRC shell
./reindex/index_all
#+END_SRC



** system setup

*** how do I control where the wikipedia dump file is downloaded?
edit your ~/.profile
change the location specified in the line
#+BEGIN_SRC shell
export GC_WIKIPEDIA_DL_DIR="/Volumes/LaCie_1/data/data_wikipedia"
#+END_SRC






* Querying

The solr system is queried over http, results can be returned in json
format or xml format.  all examples are given using the json format.

** Official documentation

[[http://wiki.apache.org/solr/CommonQueryParameters]]


** Breakdown of a query url
http://localhost:8983/solr/select/?q=articlePlainText%3A%22american%22&version=2.2&start=0&rows=1000&indent=on&wt=json

*** q parameter
The q parameter is the actual query, unurlescaped this query looks
articlePlainText:"american" . 

This tells solr to search the 'articlePlainText' field in the entire database for the term
american.

*** version parameter
The 'version' parmeter is of unknown consequence, use a value of 2.2 for
continuity.

*** start parameter
The start parameter controls the first row the result set to be
returned.

*** rows parameter
The rows parameter controls how many documents (at most) to return
after the start document.

*** indent parameter
The indent=on causes solr to pretty print the result.

*** wt parameter
The wt=json causes solr to return the result in json format.

** Interactive tour of query formation with solr
*** complex queries - phrases ANDs ORs NOTs

Take a look at [[https://github.com/General-Cybernetics/wikipedia_solr/blob/master/py/query_demo.py][py/query_demo.py]] to see this as a running program.


Note qp takes the un-urlencoded q parameter as input, it executes the
query and prints some simple stats about it, including the complete
formed url, it returns the total number of documents found for that query.

Triple quotes are a python convention for encoding multiline strings
or quote containing strings.  the value of a triple quoted string is
between the first triple quote and last triple quote.

A string preceded by a 'u' is a unicode string, for ascii only
sequences it can be thought of as a string.

The leading and trailing space in the queries are there for readability.

assert is a python statement that throws an error when it is give a
false value, none of the asserts in this tour throw an error.


****  search for american with quotes surounding
#+BEGIN_SRC py
american = qp(''' articlePlainText:"american" ''')
#+END_SRC

|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3A%22american%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|1|
|params|{u'q': u'articlePlainText:"american"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|619399|


****  search for american without surrounding quotes
#+BEGIN_SRC py
american_no_quote = qp(''' articlePlainText:american ''')
#+END_SRC
note - for single terms, we got the same number of documents back when we
quoted "american" as we got back when we didn't quote "american"

| solr url |     [[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican&start=0&rows=10&indent=on&wt=json]] |
| QTime    |                                                                                                      0 |
| params   | {u'q': u'articlePlainText:american', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'} |
| numFound |                                                                                                 619399 |

****  search for american without leading/trailing space
#+BEGIN_SRC py
american_no_trail = qp('''articlePlainText:american''')
#+END_SRC


|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican&start=0&rows=10&indent=on&wt=json]]|
|QTime|0|
|params|{u'q': u'articlePlainText:american', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|619399|


**** syntax verification
#+BEGIN_SRC py
assert american == american_no_quote
assert american_no_trail == american_no_quote
#+END_SRC


**** search for 'samoa' get 4755 docs
#+BEGIN_SRC py
samoa =qp(''' articlePlainText:samoa ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Asamoa&start=0&rows=10&indent=on&wt=json]]|
|QTime|186|
|params|{u'q': u'articlePlainText:samoa', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|4755|


****  search for 'american' or 'samoa'  get 621,927 docs
#+BEGIN_SRC py
american_or_samoa = qp(''' articlePlainText:american OR _query_:"articlePlainText:samoa" ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican+OR+_query_%3A%22articlePlainText%3Asamoa%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|191|
|params|{u'q': u'articlePlainText:american OR _query_:"articlePlainText:samoa"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|621,927|

****  search for documents containing 'american' and 'samoa' -> 2227
#+BEGIN_SRC py
american_and_samoa = qp(''' articlePlainText:american AND  _query_:"articlePlainText:samoa" ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican+AND+_query_%3A%22articlePlainText%3Asamoa%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|183|
|params|{u'q': u'articlePlainText:american AND _query_:"articlePlainText:samoa"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|2227|


**** search for docs containg 'samoa' but not containing 'american' ->2528
#+BEGIN_SRC py
samoa_not_american = qp(''' articlePlainText:samoa NOT _query_:"articlePlainText:american" ''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Asamoa+NOT+_query_%3A%22articlePlainText%3Aamerican%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|46|
|params|{u'q': u'articlePlainText:samoa NOT _query_:"articlePlainText:american"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|2528|

**** search for the phrase "american samoa" -> 1397
#+BEGIN_SRC py
american_samoa_phrase = qp(''' articlePlainText:"american samoa" ''')
#+END_SRC

|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3A%22american+samoa%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|1|
|params|{u'q': u'articlePlainText:"american samoa"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|1397|


**** proof of system consitency
#+BEGIN_SRC py
assert american_or_samoa == (american + samoa_not_american)
assert 621927            == (619399   + 2528)

assert american_and_samoa >= american_samoa_phrase
assert 2227               >=     1397
#+END_SRC

**** double phrase AND query
#+BEGIN_SRC py
 a =qp(''' articlePlainText:"american samoa" AND  _query_:"articlePlainText:'manifest destiny'" ''')
#+END_SRC

|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3A%22american+samoa%22+AND+_query_%3A%22articlePlainText%3A%27manifest+destiny%27%22&start=0&rows=10&indent=on&wt=json]]|
|QTime|199|
|params|{u'q': u'articlePlainText:"american samoa" AND _query_:"articlePlainText:\'manifest destiny\'"', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|4 |


**** ambiguous syntax
Note: the following syntax query is unclear and I can't decipher the
results, don't issue queries like this, the results are undecided  and
unsupported by me .
#+BEGIN_SRC py
a =qp('''articlePlainText:american samoa''')
#+END_SRC
|solr url|[[http://localhost:8983/solr/select/?q=articlePlainText%3Aamerican+samoa&start=0&rows=10&indent=on&wt=json]]|
|QTime|73|
|params|{u'q': u'articlePlainText:american samoa', u'start': u'0', u'wt': u'json', u'indent': u'on', u'rows': u'10'}|
|numFound|619,399|

** Additional query formation resources
If you want more information about solr query syntax, try thes resources

nested queries in solr
[[http://www.lucidimagination.com/blog/2009/03/31/nested-queries-in-solr/]]

the solr-wiki page, not actually that helpful
[[http://wiki.apache.org/solr/SolrQuerySyntax]]

* Implementation notes
These notes are meant as a guide for a future maintainer of the
solr/java search system.  They assume a knowledge of solr, java, and
common development practices.
** Overview
This wikipedia search system uses solr [[http://wiki.apache.org/solr/]]
and the jwpl wikimedia markup parsing library
[[http://code.google.com/p/jwpl/]].

I used the DataImportHandler framework to import the XML wikipedia
dump.  I wrote a custom transformer that integrates into the
DataImportHandlerFramework, this handler calls the jwpl parsing
library to extract the article text from the wikimedia markup.

I modified solr in two places.  First I changed the file reader so
that it will read from a named pipe.  This allows us to keep the
article dump compressed on disk, allowing for faster I/O and less disk
usage.

I also modified the xml reader so that it doesn't kill an entire
import if there is a missing xml tag.  This extra fault tolerance
ensures that hours of work aren't lost.  Wikipedia article dumps are
of the format described in [[http://www.mediawiki.org/xml/export-0.5.xsd]]. 
 The downloaded dumps seem to be missing the final closing
</mediawiki> tag.  We could compare md5sums if we are worried about
integrity.



*** Custom code
the custom code I wrote for this project can be found in 

**** transformer
[[https://github.com/General-Cybernetics/wikipedia_solr/blob/master/solr_home/Wikipedia_importer/wikipedia_solr/src/wikipedia_solr/WikimediaToTextTransformer.java][solr_home/Wikipedia_importer/wikipedia_solr/src/wikipedia_solr/WikimediaToTextTransformer.java]]

**** named pipe file reader
[[https://github.com/paddymul/lucene-solr/blob/06a176316bba15bf6967c87d3799ef743067e972/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileDataSource.java][
lib/solr/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileDataSource.java]]


**** tolerant xml reader
[[https://github.com/paddymul/lucene-solr/blob/06a176316bba15bf6967c87d3799ef743067e972/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java][
lib/solr/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java]]


** Solr configuration
*** Schema configuration

the solr [[ http://wiki.apache.org/solr/SchemaXml ][ schema.xml]]
for this project can be found  
[[https://github.com/General-Cybernetics/wikipedia_solr/blob/master/solr_home/solr/conf/schema.xml][
solr_home/solr/conf/schema.xml]]

**** field explanation
[[http://wiki.apache.org/solr/SchemaXml#Fields]]

each field can have one of multiple flags applied to it:
***** stored
An stored field has its original version saved by lucene.
***** indexed
An indexed field can be searched against.
**** wikipedia_solr schema

This controls which fields are stored and indexed.  We have a very
simple schema, only three relevant fields, title, articlePlainText and
sectionParsed.

***** articlePlainText
articlePlainText is the field that is searched on, it is an indexed
version of the plaintext of each wikipedia article.  It isn't stored
since the plaintext on its own isn't that useful.
***** sectionParsed
This field is stored, but not indexed.  it is a json-string
[[http://www.json.org/]] of article
sections, in the form of 

[{"section_name":["paragraph1", "paragraph2"]}, {"another section
title": ["paragph1", "p2"]}]
.



*** solr-config.xml


the [[http://wiki.apache.org/solr/SolrConfigXml][solr-config]] for this project can be found here
[[https://github.com/General-Cybernetics/wikipedia_solr/blob/master/solr_home/solr/conf/solrconfig.xml][solr_home/solr/conf/solrconfig.xml]]

It stays pretty close to the example config, except for additional
java properties that it reads, which allow the system to be more
easily configured.

*** Maintence scripts

There are a variety of maintence scripts that can be found in apps/* ,
they are explained in the Running wikipedia_solr:Apps section of this document.




